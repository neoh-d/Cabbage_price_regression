# -*- coding: utf-8 -*-
"""배추가격예측.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/neoh-d/Cabbage_price_regression/blob/main/notebook/%EB%B0%B0%EC%B6%94%EA%B0%80%EA%B2%A9%EC%98%88%EC%B8%A1.ipynb

# **1. 데이터 전처리**
"""

# 필요한 모듈
import shap
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
from IPython.display import display
from sklearn.linear_model import Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, LeaveOneOut, TimeSeriesSplit, cross_val_predict
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer

"""### ASOS(종관기상관측) 데이터"""

#. CSV 파일 읽기
df = pd.read_csv("https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/%EC%A2%85%EA%B4%80%EA%B8%B0%EC%83%81%EA%B4%80%EC%B8%A1_30.csv", encoding="utf-8")

# 1시간최다강수량은 결측치가 많아 장마데이터 피쳐로 대체
df.isna().sum()

"""
ASOS = 종관기상관측"""

df.head()

# 매핑 테이블 읽기
url = "https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/%EC%A0%84%EA%B5%AD_%EA%B4%80%EC%B8%A1%EC%A7%80%EC%A0%90%EC%BD%94%EB%93%9C%20%EC%88%98%EC%A0%95%EB%B3%B8.csv"
df_map = pd.read_csv(url, encoding='cp949')

# 원본과 병합
df_ASOS1 = df.reset_index().merge(
    df_map[['지점명','지점주소']],
    on='지점명', how='left'
)

df_ASOS1

# '지역' 컬럼 추출
df_ASOS1['지역'] = df_ASOS1['지점주소'].str.extract(r'(경기도|강원|충청북도|충청남도|전라북도|전라남도|경상북도)')[0]

# 지역만 남기고 필요없는 컬럼제거
df_ASOS2 = df_ASOS1.drop(columns=['index','지점','지점주소','1시간최다강수량(mm)','지점명'])
df_ASOS2

# 결측치 확인
df_ASOS2.isna().sum()

# 결측행이 많지 않고 퍼져있어 결측치 처리하지 않고 월과 지역으로 묶어 평균으로 바로 처리
missing_rows = df_ASOS2[df_ASOS2.isnull().any(axis=1)]
print(missing_rows)

"""ASOS = 종관기상관측"""

# 2015년도 7~9월 ASOS
df_ASOS2['일시'] = pd.to_datetime(df_ASOS2['일시'], format='%Y-%m')
df_AS15 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2015) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS15

# 1996~2024년 7~9월 ASOS
df_AS96 = df_ASOS2[(df_ASOS2['일시'].dt.year == 1996) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS97 = df_ASOS2[(df_ASOS2['일시'].dt.year == 1997) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS98 = df_ASOS2[(df_ASOS2['일시'].dt.year == 1998) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS99 = df_ASOS2[(df_ASOS2['일시'].dt.year == 1999) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS00 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2000) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS01 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2001) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS02 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2002) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS03 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2003) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS04 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2004) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS05 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2005) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS06 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2006) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS07 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2007) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS08 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2008) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS09 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2009) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS10 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2010) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS11 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2011) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS12 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2012) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS13 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2013) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS14 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2014) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS15 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2015) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS16 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2016) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS17 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2017) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS18 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2018) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS19 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2019) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS20 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2020) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS21 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2021) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS22 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2022) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS23 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2023) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]
df_AS24 = df_ASOS2[(df_ASOS2['일시'].dt.year == 2024) & (df_ASOS2['일시'].dt.month.isin(range(7,10)))]

# 평균값 처리
df15 = df_AS15.groupby(['일시', '지역'], as_index=False).mean()
df15

#1996년~2024년 ASOS ['일시','지역'] 값
df96 = df_AS96.groupby(['일시', '지역'], as_index=False).mean()
df97 = df_AS97.groupby(['일시', '지역'], as_index=False).mean()
df98 = df_AS98.groupby(['일시', '지역'], as_index=False).mean()
df99 = df_AS99.groupby(['일시', '지역'], as_index=False).mean()
df00 = df_AS00.groupby(['일시', '지역'], as_index=False).mean()
df01 = df_AS01.groupby(['일시', '지역'], as_index=False).mean()
df02 = df_AS02.groupby(['일시', '지역'], as_index=False).mean()
df03 = df_AS03.groupby(['일시', '지역'], as_index=False).mean()
df04 = df_AS04.groupby(['일시', '지역'], as_index=False).mean()
df05 = df_AS05.groupby(['일시', '지역'], as_index=False).mean()
df06 = df_AS06.groupby(['일시', '지역'], as_index=False).mean()
df07 = df_AS07.groupby(['일시', '지역'], as_index=False).mean()
df08 = df_AS08.groupby(['일시', '지역'], as_index=False).mean()
df09 = df_AS09.groupby(['일시', '지역'], as_index=False).mean()
df10 = df_AS10.groupby(['일시', '지역'], as_index=False).mean()
df11 = df_AS11.groupby(['일시', '지역'], as_index=False).mean()
df12 = df_AS12.groupby(['일시', '지역'], as_index=False).mean()
df13 = df_AS13.groupby(['일시', '지역'], as_index=False).mean()
df14 = df_AS14.groupby(['일시', '지역'], as_index=False).mean()
df15 = df_AS15.groupby(['일시', '지역'], as_index=False).mean()
df16 = df_AS16.groupby(['일시', '지역'], as_index=False).mean()
df17 = df_AS17.groupby(['일시', '지역'], as_index=False).mean()
df18 = df_AS18.groupby(['일시', '지역'], as_index=False).mean()
df19 = df_AS19.groupby(['일시', '지역'], as_index=False).mean()
df20 = df_AS20.groupby(['일시', '지역'], as_index=False).mean()
df21 = df_AS21.groupby(['일시', '지역'], as_index=False).mean()
df22 = df_AS22.groupby(['일시', '지역'], as_index=False).mean()
df23 = df_AS23.groupby(['일시', '지역'], as_index=False).mean()
df24 = df_AS24.groupby(['일시', '지역'], as_index=False).mean()

# 1996년~2024년 ASOS 병합
df_list = [df96, df97, df98, df99, df00, df01, df02, df03, df04, df05, df06, df07, df08, df09, df10, df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21, df22, df23, df24]
df_AS = pd.concat(df_list, ignore_index=True)
df_AS

# 결측치 확인
df24.isna().sum()

"""## 기후데이터/가중치 데이터"""

# 기후 데이터
df_other = pd.read_csv("https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/%EC%9E%A5%EB%A7%88%ED%8F%AD%EC%97%BC%EB%8D%B0%EC%9D%B4%ED%84%B0.CSV", encoding='utf-8')
df_other

# 가중치 데이터 (시도별+연도별 생산비율)
weight =  pd.read_csv("https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/weight_final.csv", encoding='utf-8')
weight


"""## 기상/기후/가중치 데이터 병합"""

# 기상 데이터
df = pd.read_excel("https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/df_AS.xlsx")

# 지역명 변경
df['지역'] = df['지역'].replace({
    '강원': '강원',
    '경기도': '경기',
    '경상북도': '경북',
    '전라남도': '전남',
    '충청남도': '충남',
    '충청북도': '충북',
    '전라북도': '전북'
})

# 일시 컬럼을 datetime으로 날짜 분리
df['일시'] = pd.to_datetime(df['일시'])

# 연도 컬럼 생성
df['Year'] = df['일시'].dt.year

# 연도 + 지역 기준으로 그룹화하여 평균 계산
grouped_weather = (
    df
    .groupby(['Year', '지역'], as_index=False)
    .mean(numeric_only=True)
)
# 기후 데이터 (폭염일수, 장마일수,,,)
climate_df =pd.read_excel("https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/df_other.xlsx")
climate_df = climate_df.rename(columns={'연도': 'Year'})

# grouped_weather에 climate_df 병합
weather_with_climate = pd.merge(grouped_weather, climate_df, on=['Year', '지역'], how='left')


# 가중치 데이터
weight_df = pd.read_csv("https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/weight_final.csv", encoding = 'utf-8')

# 연도, 가중치명 변경
weight_df = weight_df.rename(columns={'연도': 'Year', '가중치': 'Weight'})
weight_df['Year'] = weight_df['Year'].astype(int)

# weather + climate + weight 병합
full_df = pd.merge(weather_with_climate, weight_df[['Year', '지역', 'Weight']], on=['Year', '지역'], how='left')

# 수치형 변수 추출 (Weight와 Year 제외)
value_cols = full_df.select_dtypes(include='number').columns.difference(['Weight', 'Year'])

# 가중치를 곱한 새로운 컬럼 생성
for col in value_cols:
    full_df[col + '_weighted'] = full_df[col] * full_df['Weight']

# 연도별 가중 평균 계산
grouped = full_df.groupby('Year')
weighted_result = pd.DataFrame()
weighted_result['Weight_sum'] = grouped['Weight'].sum()

for col in value_cols:
    weighted_result[col] = grouped[col + '_weighted'].sum() / weighted_result['Weight_sum']

# 컬럼명 변경
column_rename_map = {
    '강수일수': 'Precipitation_Days',
    '평균기온(°C)': 'Avg_Temperature_C',
    '평균최고기온(°C)': 'Avg_Max_Temperature_C',
    '평균최저기온(°C)': 'Avg_Min_Temperature_C',
    '월합강수량(00~24h만)(mm)': 'Monthly_Total_Precip_mm',
    '일최다강수량(mm)': 'Max_Daily_Precip_mm',
    '합계 일조시간(hr)': 'Total_Sunshine_Hours',
    '폭염일수': 'Heatwave_Days',
    '장마일수': 'Rainy_Season_Days',
    '합계강수량': 'Total_Precipitation_mm'
}

# DataFrame에서 컬럼명 변경
df = weighted_result.rename(columns=column_rename_map)
df.drop(columns=['Weight_sum'], inplace=True)

# 원하는 순서의 컬럼 리스트
ordered_cols = [
    'Avg_Temperature_C',
    'Avg_Max_Temperature_C',
    'Avg_Min_Temperature_C',
    'Monthly_Total_Precip_mm',
    'Max_Daily_Precip_mm',
    'Total_Precipitation_mm',
    'Total_Sunshine_Hours',
    'Heatwave_Days',
    'Precipitation_Days',
    'Rainy_Season_Days'
]

other_cols = [col for col in df.columns if col not in ordered_cols]
df = df[other_cols + ordered_cols]

# 연도 + 평균가격 데이터 생성
avg_price_df = pd.DataFrame({
    'Year': [1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024],
    'avg_price': [3836,2818,3704,3870,6110,3113,3588,7699,2889,6401,4138,6051,4260,3796,8892,3368,9127,5673,4085,5120.25, 7649.5, 7117.5, 7047.25, 9501.75, 7708.5, 8169.25, 7675, 8511.5, 10773.3]
})

# 기존 데이터프레임과 병합
final_df = pd.merge(df, avg_price_df, on='Year', how='left')
final_df

"""최종 데이터 셋 다운로드"""

final_df.to_excel("final_data2.xlsx", index= False)

"""# **2. EDA (가격 추이 그래프, 상관관계 히트맵)**"""

# 1. 데이터 불러오기
df = pd.read_excel("https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/final_data2.xlsx")

# 시각화 스타일 설정
sns.set(style="whitegrid")

# 2. 연도별 평균 배추 가격 추이
plt.figure(figsize=(10, 5))
sns.lineplot(data=df, x='Year', y='avg_price', marker='o')
plt.title("Cabbage price trends by year")
plt.ylabel("avg_price")
plt.xlabel("year")
plt.tight_layout()
plt.show()

# 3. 변수 간 상관관계 히트맵
plt.figure(figsize=(12, 10))
corr = df.drop(columns='Year').corr()
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f", square=True)
plt.title("Correalation heatmap")
plt.tight_layout()
plt.show()

# 4. 모든 변수와 avg_price 간 산점도
variables = df.drop(columns=["Year", "avg_price"]).columns.tolist()
plt.figure(figsize=(18, 12))

for i, col in enumerate(variables, 1):
    plt.subplot(3, 4, i)
    sns.scatterplot(data=df, x=col, y="avg_price")
    plt.title(f"{col} vs avg_price")
    plt.tight_layout()

plt.show()

"""# **3. 모델링**"""

# 데이터 로딩
df = pd.read_excel("https://raw.githubusercontent.com/neoh-d/Cabbage_price_regression/main/data/final_data2.xlsx")
# X,y 할당
X = df.drop(columns=["Year", "avg_price"])
y = df["avg_price"]

"""## 함수"""

# 다중공선성(VIF) 분석 함수
def calculate_vif(X):
    vif_data = pd.DataFrame()
    vif_data["feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

# 회귀 평가 지표 계산 함수
def regression_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    r2 = r2_score(y_true, y_pred)
    return {
        "R²": r2,
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "MAPE (%)": mape
    }

# 모델 시각화 함수
def plot_predictions(y_true, y_pred, title=""):
    plt.figure(figsize=(6, 5))
    plt.scatter(y_true, y_pred, color='dodgerblue', edgecolor='black', label="Predicted", s=70)
    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color='darkred', linestyle='--', label="Perfect Prediction")
    plt.xlabel("Actual Price")
    plt.ylabel("Predicted Price")
    plt.title(f"{title} - Prediction vs Actual")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def plot_residuals(y_true, y_pred, title=""):
    residuals = y_true - y_pred
    plt.figure(figsize=(6, 4))
    plt.axhline(0, color='gray', linestyle='--', label="Zero Error")
    plt.scatter(range(len(y_true)), residuals, color='mediumorchid', edgecolor='black', s=70)
    plt.xlabel("Sample Index")
    plt.ylabel("Residual (Actual - Predicted)")
    plt.title(f"{title} - Residual Plot")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# 모델 학습 및 평가 함수 (전체 변수로 stepwise selection 변수 X)
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

def run_and_evaluate_model(name, model, param_grid, cv):
    grid = GridSearchCV(model, param_grid, cv=cv, scoring=mse_scorer)
    grid.fit(X, y)
    best_model = grid.best_estimator_
    y_pred = best_model.predict(X)

    # 시각화
    plot_predictions(y, y_pred, title=name)
    plot_residuals(y, y_pred, title=name)

    # 성능 지표
    metrics = regression_metrics(y, y_pred)

    return {
        "Model": name,
        "Best Params": grid.best_params_,
        **metrics
    }

# stepwise selection 함수
def stepwise_selection(X, y,
                       initial_list=[],
                       threshold_in=0.05,
                       threshold_out=0.1,
                       verbose=True):
    included = list(initial_list)
    while True:
        changed = False

        # forward step
        excluded = list(set(X.columns) - set(included))
        new_pval = pd.Series(index=excluded, dtype=float)
        for new_col in excluded:
            model = sm.OLS(y, sm.add_constant(X[included + [new_col]])).fit()
            new_pval[new_col] = model.pvalues[new_col]

        best_pval = new_pval.min()
        if best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True
            if verbose:
                print(f"Add    {best_feature:<30} (p={best_pval:.6f})")

        # backward step
        model = sm.OLS(y, sm.add_constant(X[included])).fit()
        pvalues = model.pvalues.iloc[1:]  # const 제외
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
            changed = True
            if verbose:
                print(f"Remove {worst_feature:<30} (p={worst_pval:.6f})")

        if not changed:
            break

    return included

# stepwise selection 실행
selected_features = stepwise_selection(X, y)


# 모델별 성능 요약 시각화 함수
def plot_model_performance_summary(results_df):
    metrics_df = results_df[['Model', 'R²', 'MAPE (%)']]

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    sns.barplot(data=metrics_df, x='Model', y='R²', ax=axes[0], palette='Blues_d')
    axes[0].set_title("R² by Model")
    axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')

    sns.barplot(data=metrics_df, x='Model', y='MAPE (%)', ax=axes[1], palette='Purples_d')
    axes[1].set_title("MAPE (%) by Model")
    axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')

    plt.tight_layout()
    plt.show()

"""### OLS회귀/stepwise 조건문 (feature 개수에 따라 분기)"""

# 조건 분기 처리
if len(selected_features) == 0:
    print("\n⚠️ No features were selected by stepwise selection.")

elif len(selected_features) == 1:
    print(f"\n⚠️ Only one feature selected: {selected_features[0]} → VIF calculation skipped.")

    # 최소한 회귀는 돌려보기 위함
    final_stepwise_model = sm.OLS(y, sm.add_constant(X[selected_features])).fit()
    print("\n[Stepwise Regression Summary]:")
    print(final_stepwise_model.summary())

    y_pred_stepwise = final_stepwise_model.predict(sm.add_constant(X[selected_features]))
    plot_predictions(y, y_pred_stepwise, title="Stepwise Linear Regression (1 Feature)")
    plot_residuals(y, y_pred_stepwise, title="Stepwise Linear Regression (1 Feature)")

else:
    print("\n[Stepwise Selected Features]:")
    print(selected_features)

    # VIF 계산
    vif_selected = pd.DataFrame()
    try:
        vif_selected = calculate_vif(X[selected_features])
        print("\n[VIF after Stepwise Selection]:")
        print(vif_selected.sort_values(by="VIF", ascending=False))
    except Exception as e:
        print("⚠️ Error calculating VIF:", e)


    # 최소한 회귀는 돌려보기 위함
    # 최종 회귀 적합 및 평가
    final_stepwise_model = sm.OLS(y, sm.add_constant(X[selected_features])).fit()
    print("\n[Stepwise Regression Summary]:")
    print(final_stepwise_model.summary())

    y_pred_stepwise = final_stepwise_model.predict(sm.add_constant(X[selected_features]))
    plot_predictions(y, y_pred_stepwise, title="Stepwise Linear Regression")
    plot_residuals(y, y_pred_stepwise, title="Stepwise Linear Regression")

"""###시각화 함수"""

# 중요 변수 시각화 함수: Lasso & Ridge
def plot_linear_model_coefficients(model, feature_names, title="Feature Importance (Linear Model)"):
    coefs = model.coef_
    coef_df = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': coefs
    }).sort_values(by='Coefficient', key=abs, ascending=False)

    plt.figure(figsize=(8, 5))
    plt.barh(coef_df['Feature'], coef_df['Coefficient'], color='royalblue')
    plt.xlabel("Coefficient Value")
    plt.title(title)
    plt.gca().invert_yaxis()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# 중요 변수 시각화 함수: Random Forest
def plot_rf_feature_importance(model, feature_names, title="Feature Importance (Random Forest)"):
    importances = model.feature_importances_
    imp_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)

    plt.figure(figsize=(8, 5))
    plt.barh(imp_df['Feature'], imp_df['Importance'], color='darkgreen')
    plt.xlabel("Importance Score")
    plt.title(title)
    plt.gca().invert_yaxis()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# 중요 변수 Top-N 출력 함수
def print_top_features(model, feature_names, top_n=5, model_type="linear"):
    if model_type == "linear":
        coefs = model.coef_
        imp_df = pd.DataFrame({'feature': feature_names, 'importance': np.abs(coefs)})
    elif model_type == "rf":
        imp_df = pd.DataFrame({'feature': feature_names, 'importance': model.feature_importances_})
    else:
        raise ValueError("model_type must be 'linear' or 'rf'")

    top_features = imp_df.sort_values(by='importance', ascending=False).head(top_n)
    print(f"\nTop {top_n} important features ({model_type.upper()}):\n", top_features)

# 교차검증 시각화 함수
def plot_cv_predictions(model, X, y, cv, title="Cross-validated Predictions"):
    y_cv_pred = cross_val_predict(model, X, y, cv=cv)

    plt.figure(figsize=(6, 5))
    plt.scatter(y, y_cv_pred, color='dodgerblue', edgecolor='black', label="Predicted (CV)", s=70)
    plt.plot([y.min(), y.max()], [y.min(), y.max()], color='darkred', linestyle='--', label="Perfect Prediction")
    plt.xlabel("Actual Price")
    plt.ylabel("Predicted Price (CV)")
    plt.title(title)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

  # 시각화
def plot_comparison(df):
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    sns.barplot(data=df, x='Model', y='R²', ax=axes[0], palette='coolwarm')
    axes[0].set_title("R² Comparison")
    axes[0].set_ylim(0, 1)

    sns.barplot(data=df, x='Model', y='MAPE (%)', ax=axes[1], palette='coolwarm')
    axes[1].set_title("MAPE (%) Comparison")
    axes[1].set_ylim(0, 100)

    for ax in axes:
        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')
        ax.grid(True)

    plt.tight_layout()
    plt.show()

"""##모델 하이퍼파라미터 튜닝(grid search)"""

# 파이프라인 + 하이퍼파라미터 정의
ridge_pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('ridge', Ridge())
])
lasso_pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('lasso', Lasso(max_iter=10000))
])

# 하이퍼파라미터 그리드 (pipeline 내부 이름 기반으로 지정)
ridge_pipe = Pipeline([('scaler', StandardScaler()), ('ridge', Ridge())])
lasso_pipe = Pipeline([('scaler', StandardScaler()), ('lasso', Lasso(max_iter=10000))])

param_grid_ridge = {'ridge__alpha': [0.01, 0.1, 1, 10, 100]}
param_grid_lasso = {'lasso__alpha': [0.01, 0.1, 1, 10, 100]}
param_grid_rf = {'n_estimators': [10, 50, 100], 'max_depth': [2, 4, None]}

# 검증 방법 정의
cv_loo = LeaveOneOut()
cv_ts = TimeSeriesSplit(n_splits=3)

# 모델 목록 정의
models = [
    ("Ridge (LOOCV)", ridge_pipe, param_grid_ridge, cv_loo),
    ("Lasso (LOOCV)", lasso_pipe, param_grid_lasso, cv_loo),
    ("RF (LOOCV)", RandomForestRegressor(random_state=42), param_grid_rf, cv_loo),
    ("Ridge (TS)", ridge_pipe, param_grid_ridge, cv_ts),
    ("Lasso (TS)", lasso_pipe, param_grid_lasso, cv_ts),
    ("RF (TS)", RandomForestRegressor(random_state=42), param_grid_rf, cv_ts),
]

# 전체 모델 실행 + 결과 저장
results = []
for name, model, param_grid, cv in models:
    result = run_and_evaluate_model(name, model, param_grid, cv)
    results.append(result)

# 결과 출력
results_df = pd.DataFrame(results)
print("\n Model Performance Summary:")
print(results_df)

# 모델 성능 시각화
plot_model_performance_summary(results_df)

"""##전체 변수/stepwise 변수 모델 성능 *비교*"""

# Stepwise feature subset
X_step = X[selected_features]

# 파이프라인 정의
ridge_pipe = Pipeline([('scaler', StandardScaler()), ('ridge', Ridge())])
lasso_pipe = Pipeline([('scaler', StandardScaler()), ('lasso', Lasso(max_iter=10000))])

# 재사용할 scoring
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

# 전체 변수 모델 (LOOCV)
ridge_all = GridSearchCV(ridge_pipe, param_grid_ridge, cv=cv_loo, scoring=mse_scorer)
lasso_all = GridSearchCV(lasso_pipe, param_grid_lasso, cv=cv_loo, scoring=mse_scorer)

ridge_all.fit(X, y)
lasso_all.fit(X, y)

# stepwise 변수 모델 (LOOCV)
ridge_step = GridSearchCV(ridge_pipe, param_grid_ridge, cv=cv_loo, scoring=mse_scorer)
lasso_step = GridSearchCV(lasso_pipe, param_grid_lasso, cv=cv_loo, scoring=mse_scorer)

ridge_step.fit(X_step, y)
lasso_step.fit(X_step, y)

# 예측
y_pred_ridge_all = ridge_all.best_estimator_.predict(X)
y_pred_lasso_all = lasso_all.best_estimator_.predict(X)
y_pred_ridge_step = ridge_step.best_estimator_.predict(X_step)
y_pred_lasso_step = lasso_step.best_estimator_.predict(X_step)

# 평가
ridge_all_metrics = regression_metrics(y, y_pred_ridge_all)
lasso_all_metrics = regression_metrics(y, y_pred_lasso_all)
ridge_step_metrics = regression_metrics(y, y_pred_ridge_step)
lasso_step_metrics = regression_metrics(y, y_pred_lasso_step)

# 결과 테이블
comparison_df = pd.DataFrame([
    {"Model": "Ridge (All)", **ridge_all_metrics},
    {"Model": "Lasso (All)", **lasso_all_metrics},
    {"Model": "Ridge (Stepwise)", **ridge_step_metrics},
    {"Model": "Lasso (Stepwise)", **lasso_step_metrics},
])

print("\n[전체 변수 vs Stepwise 변수 성능 비교]")
print(comparison_df)

"""###best model 중요 변수 시각화

"""

# Ridge: alpha=1 (from LOOCV,TS best result)
ridge_best_LOOCV = Ridge(alpha=1)
ridge_best_LOOCV.fit(X, y)
plot_linear_model_coefficients(
    model=ridge_best_LOOCV,
    feature_names=X.columns,
    title="Ridge Feature Importance (alpha=1)"
)


# Random Forest: max_depth=2, n_estimators=50 (LOOCV best)
rf_best_LOOCV = RandomForestRegressor(n_estimators=50, max_depth=2, random_state=42)
rf_best_LOOCV.fit(X, y)
plot_rf_feature_importance(
    model=rf_best_LOOCV,
    feature_names=X.columns,
    title="Random Forest Feature Importance (depth=2, trees=50)"
)

# Random Forest: max_depth=4, n_estimators=50 (TS best)
rf_best_TS = RandomForestRegressor(n_estimators=50, max_depth=4, random_state=42)
rf_best_TS.fit(X, y)
plot_rf_feature_importance(
    model=rf_best_TS,
    feature_names=X.columns,
    title="Random Forest Feature Importance (depth=4, trees=50)"
)


# 전체 변수 vs stepwise 변수
plot_comparison(comparison_df)

"""###전체 변수/stepwise 변수 예측 시각화 (교차검증)"""

# 교차검증 기반 예측 시각화(전체 변수)
plot_cv_predictions(ridge_best_LOOCV, X, y, cv=LeaveOneOut(), title="Ridge α=1 - CV Prediction")
plot_cv_predictions(rf_best_LOOCV, X, y, cv=LeaveOneOut(), title="Random Forest - CV Prediction")
plot_cv_predictions(rf_best_TS, X, y, cv=LeaveOneOut(), title="Random Forest - Ts Prediction")

# Stepwise 변수만 추출
X_step = X[selected_features]

# Stepwise 변수 기반 예측 시각화
plot_cv_predictions(ridge_best_LOOCV, X_step, y, cv=LeaveOneOut(), title="Ridge α=1 - Stepwise - CV/TS Prediction")
plot_cv_predictions(rf_best_LOOCV, X_step, y, cv=LeaveOneOut(), title="Random Forest - Stepwise - CV Prediction")
plot_cv_predictions(rf_best_TS, X_step, y, cv=LeaveOneOut(), title="Random Forest - Stepwise - TS Prediction")

"""###best model 중요 변수 시각화

### 중요 변수 Top-5
"""

# 중요 변수 Top-5 출력
print_top_features(ridge_best_LOOCV, X.columns, top_n=5, model_type="linear")
print_top_features(rf_best_LOOCV, X.columns, top_n=5, model_type="rf")
print_top_features(rf_best_TS, X.columns, top_n=5, model_type="rf")

"""## Top-5 변수 리모델링


"""

# 중요 변수 Top 5 (각 모델별)

# Linear 모델의 첫 번째 결과 (맨 위)
top5_ridge = ['Avg_Min_Temperature_C', 'Avg_Max_Temperature_C', 'Avg_Temperature_C', 'Heatwave_Days', 'Max_Daily_Precip_mm']

# RandomForest 모델의 첫 번째 결과
top5_rf1 = ['Avg_Min_Temperature_C', 'Avg_Temperature_C', 'Monthly_Total_Precip_mm', 'Max_Daily_Precip_mm', 'Total_Sunshine_Hours']

# RandomForest 모델의 두 번째 결과
top5_rf2 = ['Avg_Min_Temperature_C', 'Avg_Temperature_C', 'Monthly_Total_Precip_mm', 'Rainy_Season_Days', 'Avg_Max_Temperature_C']


# 데이터 서브셋
X_ridge_top5 = X[top5_ridge]
X_rf1_top5 = X[top5_rf1]
X_rf2_top5 = X[top5_rf2]

# 파이프라인 정의
ridge_pipe = Pipeline([('scaler', StandardScaler()), ('ridge', Ridge())])
lasso_pipe = Pipeline([('scaler', StandardScaler()), ('lasso', Lasso(max_iter=10000))])
rf_model = RandomForestRegressor(random_state=42)

# 그리드서치 설정
ridge_grid = GridSearchCV(ridge_pipe, param_grid_ridge, cv=LeaveOneOut(), scoring=mse_scorer)
rf_grid = GridSearchCV(rf_model, param_grid_rf, cv=LeaveOneOut(), scoring=mse_scorer)
rf_grid2 = GridSearchCV(rf_model, param_grid_rf, cv=LeaveOneOut(), scoring=mse_scorer)

# 학습
ridge_grid.fit(X_ridge_top5, y)
rf_grid.fit(X_rf1_top5, y)
rf_grid2.fit(X_rf2_top5, y)

# 예측
y_pred_ridge1_top5 = ridge_grid.best_estimator_.predict(X_ridge_top5)
y_pred_rf1_top5 = rf_grid.best_estimator_.predict(X_rf1_top5)
y_pred_rf2_top5 = rf_grid2.best_estimator_.predict(X_rf2_top5)

# 성능 평가
ridge1_metrics = regression_metrics(y, y_pred_ridge1_top5)
rf1_metrics = regression_metrics(y, y_pred_rf1_top5)
rf2_metrics = regression_metrics(y, y_pred_rf2_top5)

# 비교 테이블 출력
top5_comparison_df = pd.DataFrame([
    {"Model": "Ridge_CV (Top 5)", **ridge1_metrics},
    {"Model": "RF_CV (Top 5)", **rf1_metrics},
    {"Model": "RF_TS (Top 5)", **rf2_metrics},
])

print("\n[중요 변수 Top 5 기반 모델 성능 비교]")
print(top5_comparison_df)

# 모델 성능 시각화
plot_model_performance_summary(top5_comparison_df)

"""# **4. 분석**

## SHAP
"""

# SHAP 분석 함수
def run_shap_analysis_multiple(model, X_input, feature_set_name, top_features, y_true=None, show_force=True):
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_input)
    y_pred = model.predict(X_input)

    # Summary Plot (dot)
    print(f"\n SHAP Summary Plot: {feature_set_name}")
    shap.summary_plot(shap_values, X_input, plot_type="dot")

    # Dependence Plot (Top 변수 반복)
    for feature in top_features:
        print(f" SHAP Dependence Plot for: {feature}")
        shap.dependence_plot(feature, shap_values, X_input)

    # Force plots
    if show_force:
        shap.initjs()

        print(" SHAP Force Plot - 1st Sample (index=0)")
        display(shap.force_plot(explainer.expected_value, shap_values[0], X_input.iloc[0], feature_names=X_input.columns))

        i_max_pred = y_pred.argmax()
        print(f" SHAP Force Plot - Highest Prediction (index={i_max_pred})")
        display(shap.force_plot(explainer.expected_value, shap_values[i_max_pred], X_input.iloc[i_max_pred], feature_names=X_input.columns))

        i_min_pred = y_pred.argmin()
        print(f" SHAP Force Plot - Lowest Prediction (index={i_min_pred})")
        display(shap.force_plot(explainer.expected_value, shap_values[i_min_pred], X_input.iloc[i_min_pred], feature_names=X_input.columns))

        if y_true is not None:
            errors = np.abs(y_pred - y_true)
            i_max_error = errors.argmax()
            print(f" SHAP Force Plot - Max Error (index={i_max_error})")
            display(shap.force_plot(explainer.expected_value, shap_values[i_max_error], X_input.iloc[i_max_error], feature_names=X_input.columns))

    # 반환값 추가: SHAP 평균 중요도
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    return pd.DataFrame({
        "Feature": X_input.columns,
        "MeanSHAP": mean_abs_shap,
        "Model": feature_set_name
    })

# top5 변수의 인덱스를 기준으로 y에서 같은 행 추출
y_rf1_top5 = y.loc[X_rf1_top5.index]
y_rf2_top5 = y.loc[X_rf2_top5.index]


# 모델 정의 (이미 학습된 모델 사용)
# - rf_grid.best_estimator_: Top5 기반 CV
# - rf_grid2.best_estimator_: Top5 기반 TS
# - rf_best_LOOCV: 전체 변수 CV 기반
# - rf_best_TS: 전체 변수 TS 기반

# Top 5 기반 모델들 (Top 5/전체 변수)
df_shap_1 = run_shap_analysis_multiple(rf_grid.best_estimator_, X_rf1_top5, "Top 5 RF (CV)", top5_rf1, y_true=y_rf1_top5)
df_shap_2 = run_shap_analysis_multiple(rf_grid2.best_estimator_, X_rf2_top5, "Top 5 RF (TS)", top5_rf2, y_true=y_rf2_top5)
df_shap_3 = run_shap_analysis_multiple(rf_best_LOOCV, X, "All Features RF (CV)", X.columns, y_true=y)
df_shap_4 = run_shap_analysis_multiple(rf_best_TS, X, "All Features RF (TS)", X.columns, y_true=y)

# 통합
df_all_shap = pd.concat([df_shap_1, df_shap_2, df_shap_3, df_shap_4])

"""### **SHAP 평균 중요도 비교 시각화**

  <해석 팁>

-막대가 긴 feature일수록 해당 모델에서 예측에 많이 기여

-같은 feature라도 모델마다 중요도가 다를 수 있음 → 모델 해석에 중요

-CV와 TS 모델 간 중요도 차이 → 모델의 민감도 또는 해석 차이를 반영


"""

# 상위 N개 feature 기준 선택
top_features = df_all_shap.groupby("Feature")["MeanSHAP"].mean().sort_values(ascending=False).head(10).index

# 시각화용 pivot
df_plot = df_all_shap[df_all_shap["Feature"].isin(top_features)]
df_plot = df_plot.pivot(index="Feature", columns="Model", values="MeanSHAP").fillna(0)

# 시각화
df_plot.plot(kind="barh", figsize=(10, 6), title="SHAP Feature Importance Comparison")
plt.xlabel("Mean Absolute SHAP Value")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()